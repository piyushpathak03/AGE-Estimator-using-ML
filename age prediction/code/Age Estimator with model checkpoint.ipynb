{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Handle table-like data and matrices\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "\n",
    "# utils\n",
    "# from scipy.misc import imread\n",
    "# from scipy.misc import imresize\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Configure visualisations\n",
    "%matplotlib inline\n",
    "color = sns.color_palette()\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999\n",
    "mpl.style.use( 'ggplot' )\n",
    "sns.set_style( 'white' )\n",
    "pylab.rcParams[ 'figure.figsize' ] = 10,8 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the root path\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "root_dir = os.path.abspath('.')\n",
    "# set the data dir\n",
    "data_dir = '../input'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import train and test csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n",
    "test = pd.read_csv(os.path.join(data_dir, 'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_name</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100_1_0_20170110183726390.jpg.chip.jpg</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100_1_2_20170105174847679.jpg.chip.jpg</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101_1_2_20170105174739309.jpg.chip.jpg</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10_0_0_20161220222308131.jpg.chip.jpg</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10_0_0_20170103200329407.jpg.chip.jpg</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                File_name  Age\n",
       "0  100_1_0_20170110183726390.jpg.chip.jpg  100\n",
       "1  100_1_2_20170105174847679.jpg.chip.jpg  100\n",
       "2  101_1_2_20170105174739309.jpg.chip.jpg  101\n",
       "3   10_0_0_20161220222308131.jpg.chip.jpg   10\n",
       "4   10_0_0_20170103200329407.jpg.chip.jpg   10"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_name</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_0_0_20170103200522151.jpg.chip.jpg</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_0_0_20170110220447314.jpg.chip.jpg</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10_0_0_20170110221714752.jpg.chip.jpg</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10_0_0_20170110224524253.jpg.chip.jpg</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10_0_0_20170110224725285.jpg.chip - Copy.jpg</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      File_name  Age\n",
       "0         10_0_0_20170103200522151.jpg.chip.jpg   10\n",
       "1         10_0_0_20170110220447314.jpg.chip.jpg   10\n",
       "2         10_0_0_20170110221714752.jpg.chip.jpg   10\n",
       "3         10_0_0_20170110224524253.jpg.chip.jpg   10\n",
       "4  10_0_0_20170110224725285.jpg.chip - Copy.jpg   10"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape : (8255, 2)\n",
      "Test shape : (1635, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train shape :\",train.shape)\n",
    "print(\"Test shape :\",test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_image(df, folder_name):\n",
    "    i = random.choice(df.index)\n",
    "\n",
    "    img_name = df.File_name[i]\n",
    "    img = imread(os.path.join(data_dir, folder_name, img_name))\n",
    "\n",
    "    print('Age : ', train.Age[i])\n",
    "    imshow(img)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age :  39\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "imshow() missing required argument 'mat' (pos 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-bea55686af05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimread\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mshow_random_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"train\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-7bbf7e13f420>\u001b[0m in \u001b[0;36mshow_random_image\u001b[1;34m(df, folder_name)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Age : '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAge\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: imshow() missing required argument 'mat' (pos 2)"
     ]
    }
   ],
   "source": [
    "# show random image \n",
    "import random\n",
    "from cv2 import imread,imshow\n",
    "\n",
    "show_random_image(train, \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def resize_img(df, folder_name):\n",
    "    temp = []\n",
    "    for index,img_name in enumerate(df.File_name):\n",
    "        if index % 1000 == 0:\n",
    "            print( 'Index :',index)\n",
    "        img_path = os.path.join(data_dir, folder_name, img_name)\n",
    "        img = imread(img_path)\n",
    "        img = cv2.resize(img, (32, 32))\n",
    "        img = img.astype('float32')\n",
    "        temp.append(img)\n",
    "\n",
    "    return np.stack(temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index : 0\n",
      "Index : 1000\n",
      "Index : 2000\n",
      "Index : 3000\n",
      "Index : 4000\n",
      "Index : 5000\n",
      "Index : 6000\n",
      "Index : 7000\n",
      "Index : 8000\n",
      "Wall time: 8.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# resize all the images 32 x 32 (training set), usually takes ~ 4-5 min (CPU)\n",
    "train_x = resize_img(train, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index : 0\n",
      "Index : 1000\n"
     ]
    }
   ],
   "source": [
    "# resize all the images 32 x 32 (testing set)\n",
    "test_x = resize_img(test, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.95686275, 0.91764706, 0.8862745 ],\n",
       "         [0.57254905, 0.4627451 , 0.39607844],\n",
       "         [0.46666667, 0.36862746, 0.30980393],\n",
       "         ...,\n",
       "         [0.14901961, 0.1254902 , 0.10980392],\n",
       "         [0.12941177, 0.09019608, 0.09019608],\n",
       "         [0.9137255 , 0.88235295, 0.85490197]],\n",
       "\n",
       "        [[0.7058824 , 0.6392157 , 0.6       ],\n",
       "         [0.63529414, 0.50980395, 0.45490196],\n",
       "         [0.45882353, 0.3882353 , 0.3647059 ],\n",
       "         ...,\n",
       "         [0.07450981, 0.0627451 , 0.07450981],\n",
       "         [0.17254902, 0.12941177, 0.13725491],\n",
       "         [0.8235294 , 0.7921569 , 0.7647059 ]],\n",
       "\n",
       "        [[0.4862745 , 0.38431373, 0.33333334],\n",
       "         [0.4862745 , 0.34901962, 0.3019608 ],\n",
       "         [0.20784314, 0.16862746, 0.16470589],\n",
       "         ...,\n",
       "         [0.02352941, 0.03921569, 0.07058824],\n",
       "         [0.1764706 , 0.13725491, 0.13725491],\n",
       "         [0.74509805, 0.7176471 , 0.6862745 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.7137255 , 0.4117647 , 0.16078432],\n",
       "         [0.6666667 , 0.4392157 , 0.24313726],\n",
       "         [0.46666667, 0.26666668, 0.09019608],\n",
       "         ...,\n",
       "         [0.05098039, 0.04313726, 0.04313726],\n",
       "         [0.07843138, 0.07058824, 0.07058824],\n",
       "         [0.03529412, 0.04313726, 0.04313726]],\n",
       "\n",
       "        [[0.30980393, 0.21568628, 0.15294118],\n",
       "         [0.37254903, 0.21960784, 0.11764706],\n",
       "         [0.34117648, 0.18039216, 0.07058824],\n",
       "         ...,\n",
       "         [0.05490196, 0.03137255, 0.03529412],\n",
       "         [0.07058824, 0.04313726, 0.04705882],\n",
       "         [0.18431373, 0.07450981, 0.04705882]],\n",
       "\n",
       "        [[0.46666667, 0.26666668, 0.12941177],\n",
       "         [0.5882353 , 0.32941177, 0.12156863],\n",
       "         [0.67058825, 0.36078432, 0.09803922],\n",
       "         ...,\n",
       "         [0.03137255, 0.03137255, 0.03137255],\n",
       "         [0.1254902 , 0.12156863, 0.12156863],\n",
       "         [0.29803923, 0.2784314 , 0.2627451 ]]],\n",
       "\n",
       "\n",
       "       [[[0.09803922, 0.13333334, 0.17254902],\n",
       "         [0.09019608, 0.1254902 , 0.16470589],\n",
       "         [0.10196079, 0.12941177, 0.16862746],\n",
       "         ...,\n",
       "         [0.15686275, 0.20784314, 0.26666668],\n",
       "         [0.23921569, 0.2784314 , 0.31764707],\n",
       "         [0.27450982, 0.32156864, 0.34509805]],\n",
       "\n",
       "        [[0.08627451, 0.10980392, 0.13725491],\n",
       "         [0.08235294, 0.10588235, 0.13333334],\n",
       "         [0.03529412, 0.0627451 , 0.09803922],\n",
       "         ...,\n",
       "         [0.14117648, 0.1882353 , 0.25882354],\n",
       "         [0.18431373, 0.21960784, 0.27058825],\n",
       "         [0.24705882, 0.28627452, 0.3254902 ]],\n",
       "\n",
       "        [[0.0627451 , 0.07843138, 0.09803922],\n",
       "         [0.06666667, 0.08235294, 0.10196079],\n",
       "         [0.01176471, 0.04313726, 0.07843138],\n",
       "         ...,\n",
       "         [0.16470589, 0.20784314, 0.2901961 ],\n",
       "         [0.15686275, 0.19215687, 0.25490198],\n",
       "         [0.19607843, 0.23137255, 0.28235295]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.15294118, 0.20392157, 0.21176471],\n",
       "         [0.06666667, 0.10196079, 0.11372549],\n",
       "         [0.05098039, 0.07843138, 0.08627451],\n",
       "         ...,\n",
       "         [0.16862746, 0.2       , 0.22745098],\n",
       "         [0.16862746, 0.22745098, 0.2627451 ],\n",
       "         [0.20784314, 0.26666668, 0.3019608 ]],\n",
       "\n",
       "        [[0.2       , 0.25490198, 0.2509804 ],\n",
       "         [0.10588235, 0.14509805, 0.14509805],\n",
       "         [0.07058824, 0.10196079, 0.09803922],\n",
       "         ...,\n",
       "         [0.1882353 , 0.22352941, 0.23921569],\n",
       "         [0.19215687, 0.25490198, 0.2784314 ],\n",
       "         [0.21176471, 0.27058825, 0.30588236]],\n",
       "\n",
       "        [[0.26666668, 0.3137255 , 0.32156864],\n",
       "         [0.10980392, 0.14117648, 0.14117648],\n",
       "         [0.08627451, 0.10980392, 0.10588235],\n",
       "         ...,\n",
       "         [0.22745098, 0.25490198, 0.2901961 ],\n",
       "         [0.24313726, 0.2901961 , 0.32941177],\n",
       "         [0.23529412, 0.29411766, 0.32941177]]],\n",
       "\n",
       "\n",
       "       [[[0.8627451 , 0.9019608 , 0.81960785],\n",
       "         [0.85490197, 0.91764706, 0.827451  ],\n",
       "         [0.8745098 , 0.9019608 , 0.81960785],\n",
       "         ...,\n",
       "         [0.8117647 , 0.8       , 0.8627451 ],\n",
       "         [0.44705883, 0.4117647 , 0.39607844],\n",
       "         [0.47843137, 0.3882353 , 0.33333334]],\n",
       "\n",
       "        [[0.84313726, 0.8980392 , 0.8117647 ],\n",
       "         [0.84705883, 0.9019608 , 0.8156863 ],\n",
       "         [0.91764706, 0.9098039 , 0.83137256],\n",
       "         ...,\n",
       "         [0.8627451 , 0.84313726, 0.9647059 ],\n",
       "         [0.8039216 , 0.7607843 , 0.81960785],\n",
       "         [0.38039216, 0.29803923, 0.3019608 ]],\n",
       "\n",
       "        [[0.84313726, 0.9098039 , 0.81960785],\n",
       "         [0.85490197, 0.9019608 , 0.8156863 ],\n",
       "         [0.9137255 , 0.8862745 , 0.8117647 ],\n",
       "         ...,\n",
       "         [0.85882354, 0.8352941 , 0.99215686],\n",
       "         [0.81960785, 0.77254903, 0.8980392 ],\n",
       "         [0.6862745 , 0.61960787, 0.65882355]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.79607844, 0.88235295, 0.77254903],\n",
       "         [0.7921569 , 0.8627451 , 0.7411765 ],\n",
       "         [0.7764706 , 0.84313726, 0.7137255 ],\n",
       "         ...,\n",
       "         [0.52156866, 0.54509807, 0.6       ],\n",
       "         [0.6509804 , 0.6666667 , 0.6392157 ],\n",
       "         [0.80784315, 0.8235294 , 0.7058824 ]],\n",
       "\n",
       "        [[0.76862746, 0.8784314 , 0.7764706 ],\n",
       "         [0.78039217, 0.85490197, 0.7490196 ],\n",
       "         [0.7764706 , 0.83137256, 0.72156864],\n",
       "         ...,\n",
       "         [0.4       , 0.41568628, 0.44705883],\n",
       "         [0.46666667, 0.47843137, 0.41960785],\n",
       "         [0.32156864, 0.34117648, 0.21568628]],\n",
       "\n",
       "        [[0.79607844, 0.88235295, 0.78431374],\n",
       "         [0.76862746, 0.8235294 , 0.7294118 ],\n",
       "         [0.80784315, 0.827451  , 0.7372549 ],\n",
       "         ...,\n",
       "         [0.3019608 , 0.29803923, 0.30588236],\n",
       "         [0.27450982, 0.27450982, 0.21568628],\n",
       "         [0.27450982, 0.2509804 , 0.17254902]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0.7647059 , 0.8235294 , 0.8509804 ],\n",
       "         [0.7058824 , 0.7882353 , 0.7882353 ],\n",
       "         [0.31764707, 0.37254903, 0.35686275],\n",
       "         ...,\n",
       "         [0.23137255, 0.2784314 , 0.2784314 ],\n",
       "         [0.7411765 , 0.8392157 , 0.88235295],\n",
       "         [0.7764706 , 0.85882354, 0.9137255 ]],\n",
       "\n",
       "        [[0.7647059 , 0.83137256, 0.84313726],\n",
       "         [0.6509804 , 0.72156864, 0.70980394],\n",
       "         [0.22745098, 0.26666668, 0.24313726],\n",
       "         ...,\n",
       "         [0.21176471, 0.2627451 , 0.25490198],\n",
       "         [0.7058824 , 0.80784315, 0.83137256],\n",
       "         [0.7607843 , 0.85882354, 0.8745098 ]],\n",
       "\n",
       "        [[0.7176471 , 0.77254903, 0.78431374],\n",
       "         [0.45882353, 0.50980395, 0.49411765],\n",
       "         [0.28235295, 0.29803923, 0.28235295],\n",
       "         ...,\n",
       "         [0.16862746, 0.21960784, 0.21960784],\n",
       "         [0.7137255 , 0.8235294 , 0.827451  ],\n",
       "         [0.7372549 , 0.8235294 , 0.8666667 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.7647059 , 0.79607844, 0.68235296],\n",
       "         [0.39607844, 0.44705883, 0.7411765 ],\n",
       "         [0.37254903, 0.49019608, 0.7294118 ],\n",
       "         ...,\n",
       "         [0.6901961 , 0.8235294 , 0.827451  ],\n",
       "         [0.6784314 , 0.8117647 , 0.8117647 ],\n",
       "         [0.47843137, 0.5294118 , 0.5372549 ]],\n",
       "\n",
       "        [[0.5372549 , 0.6117647 , 0.5411765 ],\n",
       "         [0.41568628, 0.5529412 , 0.7647059 ],\n",
       "         [0.43529412, 0.4862745 , 0.7921569 ],\n",
       "         ...,\n",
       "         [0.69411767, 0.8156863 , 0.8117647 ],\n",
       "         [0.69411767, 0.8156863 , 0.8117647 ],\n",
       "         [0.37254903, 0.42745098, 0.43137255]],\n",
       "\n",
       "        [[0.5803922 , 0.67058825, 0.5803922 ],\n",
       "         [0.43137255, 0.5294118 , 0.78039217],\n",
       "         [0.43529412, 0.5019608 , 0.827451  ],\n",
       "         ...,\n",
       "         [0.63529414, 0.7607843 , 0.74509805],\n",
       "         [0.6666667 , 0.8       , 0.8       ],\n",
       "         [0.01176471, 0.01568628, 0.02352941]]],\n",
       "\n",
       "\n",
       "       [[[0.03137255, 0.03529412, 0.02745098],\n",
       "         [0.03529412, 0.03529412, 0.02745098],\n",
       "         [0.05490196, 0.05882353, 0.05098039],\n",
       "         ...,\n",
       "         [0.06666667, 0.07058824, 0.08627451],\n",
       "         [0.03921569, 0.06666667, 0.07843138],\n",
       "         [0.1254902 , 0.14509805, 0.14901961]],\n",
       "\n",
       "        [[0.03529412, 0.04313726, 0.03529412],\n",
       "         [0.01960784, 0.03921569, 0.04313726],\n",
       "         [0.01568628, 0.04705882, 0.07450981],\n",
       "         ...,\n",
       "         [0.08235294, 0.09411765, 0.1254902 ],\n",
       "         [0.0627451 , 0.08235294, 0.08627451],\n",
       "         [0.04705882, 0.06666667, 0.0627451 ]],\n",
       "\n",
       "        [[0.03529412, 0.03137255, 0.01568628],\n",
       "         [0.01568628, 0.04313726, 0.05490196],\n",
       "         [0.09019608, 0.14117648, 0.20784314],\n",
       "         ...,\n",
       "         [0.05098039, 0.0627451 , 0.09411765],\n",
       "         [0.02352941, 0.04313726, 0.05490196],\n",
       "         [0.03529412, 0.03921569, 0.02745098]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.10588235, 0.10196079, 0.08627451],\n",
       "         [0.07843138, 0.07450981, 0.05882353],\n",
       "         [0.07058824, 0.08235294, 0.05882353],\n",
       "         ...,\n",
       "         [0.2784314 , 0.19607843, 0.27058825],\n",
       "         [0.1254902 , 0.15294118, 0.10196079],\n",
       "         [0.18039216, 0.17254902, 0.12941177]],\n",
       "\n",
       "        [[0.10980392, 0.10588235, 0.09019608],\n",
       "         [0.09411765, 0.09019608, 0.07450981],\n",
       "         [0.09411765, 0.09019608, 0.07450981],\n",
       "         ...,\n",
       "         [0.17254902, 0.16862746, 0.11372549],\n",
       "         [0.17254902, 0.14509805, 0.13333334],\n",
       "         [0.16078432, 0.15294118, 0.10980392]],\n",
       "\n",
       "        [[0.09019608, 0.09019608, 0.06666667],\n",
       "         [0.10588235, 0.10588235, 0.08235294],\n",
       "         [0.07843138, 0.08235294, 0.04705882],\n",
       "         ...,\n",
       "         [0.16862746, 0.15686275, 0.1254902 ],\n",
       "         [0.14901961, 0.13725491, 0.10588235],\n",
       "         [0.16862746, 0.15686275, 0.1254902 ]]],\n",
       "\n",
       "\n",
       "       [[[0.54901963, 0.654902  , 0.91764706],\n",
       "         [0.32941177, 0.4       , 0.54509807],\n",
       "         [0.20784314, 0.23529412, 0.30980393],\n",
       "         ...,\n",
       "         [0.8156863 , 0.85882354, 0.8862745 ],\n",
       "         [0.95686275, 0.98039216, 0.98039216],\n",
       "         [0.9843137 , 0.9764706 , 0.99215686]],\n",
       "\n",
       "        [[0.5019608 , 0.6       , 0.87058824],\n",
       "         [0.27058825, 0.32156864, 0.5019608 ],\n",
       "         [0.17254902, 0.18039216, 0.29411766],\n",
       "         ...,\n",
       "         [0.4509804 , 0.5372549 , 0.6       ],\n",
       "         [0.92156863, 0.972549  , 0.9647059 ],\n",
       "         [0.98039216, 0.96862745, 0.98039216]],\n",
       "\n",
       "        [[0.44705883, 0.56078434, 0.8666667 ],\n",
       "         [0.4862745 , 0.5529412 , 0.8039216 ],\n",
       "         [0.09803922, 0.1254902 , 0.2901961 ],\n",
       "         ...,\n",
       "         [0.28235295, 0.37254903, 0.5176471 ],\n",
       "         [0.8666667 , 0.90588236, 0.99607843],\n",
       "         [0.92941177, 0.9529412 , 1.        ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.54901963, 0.6313726 , 0.8980392 ],\n",
       "         [0.5568628 , 0.6039216 , 0.89411765],\n",
       "         [0.5294118 , 0.5568628 , 0.8666667 ],\n",
       "         ...,\n",
       "         [0.5411765 , 0.61960787, 0.90588236],\n",
       "         [0.54509807, 0.64705884, 0.91764706],\n",
       "         [0.5411765 , 0.6745098 , 0.92156863]],\n",
       "\n",
       "        [[0.08627451, 0.1254902 , 0.34117648],\n",
       "         [0.09411765, 0.09019608, 0.34509805],\n",
       "         [0.11372549, 0.08235294, 0.38039216],\n",
       "         ...,\n",
       "         [0.05098039, 0.07843138, 0.23921569],\n",
       "         [0.1882353 , 0.22745098, 0.41568628],\n",
       "         [0.28627452, 0.36862746, 0.6117647 ]],\n",
       "\n",
       "        [[0.11764706, 0.09411765, 0.2509804 ],\n",
       "         [0.2627451 , 0.17254902, 0.40784314],\n",
       "         [0.24705882, 0.11764706, 0.40784314],\n",
       "         ...,\n",
       "         [0.04705882, 0.03921569, 0.1254902 ],\n",
       "         [0.04313726, 0.03137255, 0.13333334],\n",
       "         [0.03921569, 0.06666667, 0.13725491]]]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatten the image array\n",
    "train_x = train_x / 255\n",
    "test_x = test_x / 255\n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import utils\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "train_y = train.Age\n",
    "train_y = to_categorical(train_y)\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the parameters for neural network\n",
    "input_num_units = (32, 32, 3)\n",
    "hidden_num_units = 600\n",
    "output_num_units = 3\n",
    "num_filter = 100\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MOdeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, InputLayer\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 30, 30, 100)       2800      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 15, 15, 100)       0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 15, 15, 100)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 13, 13, 50)        45050     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 50)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6, 6, 50)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 30)          13530     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 480)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 600)               288600    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               72120     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 121       \n",
      "=================================================================\n",
      "Total params: 422,221\n",
      "Trainable params: 422,221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "## Input Layer\n",
    "model.add(InputLayer(input_shape=input_num_units))\n",
    "\n",
    "## Convolutional layer\n",
    "model.add(Conv2D(num_filter, (3, 3), activation='relu', input_shape=input_num_units))\n",
    "\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(int(num_filter * 0.5), (3, 3), activation='relu', input_shape=(10, 10, 100)))\n",
    "\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(int(num_filter * 0.3), (3, 3), activation='relu', input_shape=(1, 1, 10)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(hidden_num_units, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(int(hidden_num_units * 0.2), activation='relu'))\n",
    "\n",
    "model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0103 - mean_absolute_error: 0.0103\n",
      "Epoch 00001: val_loss improved from inf to 0.00919, saving model to Weights-001--0.00919.hdf5\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 0.0103 - mean_absolute_error: 0.0103 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092\n",
      "Epoch 2/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00002: val_loss improved from 0.00919 to 0.00906, saving model to Weights-002--0.00906.hdf5\n",
      "117/117 [==============================] - 12s 106ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 3/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00003: val_loss did not improve from 0.00906\n",
      "117/117 [==============================] - 14s 118ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 4/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00004: val_loss improved from 0.00906 to 0.00906, saving model to Weights-004--0.00906.hdf5\n",
      "117/117 [==============================] - 16s 139ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 5/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00005: val_loss did not improve from 0.00906\n",
      "117/117 [==============================] - 17s 147ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 6/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
      "Epoch 00006: val_loss improved from 0.00906 to 0.00903, saving model to Weights-006--0.00903.hdf5\n",
      "117/117 [==============================] - 18s 151ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090\n",
      "Epoch 7/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00007: val_loss did not improve from 0.00903\n",
      "117/117 [==============================] - 16s 141ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 8/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00008: val_loss did not improve from 0.00903\n",
      "117/117 [==============================] - 17s 143ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 9/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00009: val_loss did not improve from 0.00903\n",
      "117/117 [==============================] - 16s 140ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 10/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
      "Epoch 00010: val_loss did not improve from 0.00903\n",
      "117/117 [==============================] - 15s 129ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 11/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00011: val_loss did not improve from 0.00903\n",
      "117/117 [==============================] - 15s 129ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 12/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00012: val_loss improved from 0.00903 to 0.00901, saving model to Weights-012--0.00901.hdf5\n",
      "117/117 [==============================] - 14s 122ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090\n",
      "Epoch 13/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00013: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 14s 118ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 14/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00014: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 14s 116ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 15/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00015: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 15s 129ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0094 - val_mean_absolute_error: 0.0094\n",
      "Epoch 16/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00016: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 16s 136ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 17/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00017: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 15s 129ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 18/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00018: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 15s 128ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090\n",
      "Epoch 19/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00019: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 15s 127ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 20/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00020: val_loss improved from 0.00901 to 0.00901, saving model to Weights-020--0.00901.hdf5\n",
      "117/117 [==============================] - 15s 128ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090\n",
      "Epoch 21/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00021: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 15s 128ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090\n",
      "Epoch 22/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
      "Epoch 00022: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 14s 124ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092\n",
      "Epoch 23/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00023: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 17s 144ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 24/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00024: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 17s 147ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090\n",
      "Epoch 25/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
      "Epoch 00025: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 18s 155ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092\n",
      "Epoch 26/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00026: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 17s 142ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092\n",
      "Epoch 27/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00027: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 17s 144ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090\n",
      "Epoch 28/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00028: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 16s 133ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 29/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00029: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 16s 133ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092\n",
      "Epoch 30/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00030: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 16s 138ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090\n",
      "Epoch 31/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00031: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 15s 128ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 32/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00032: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 15s 125ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092\n",
      "Epoch 33/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00033: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 16s 138ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092\n",
      "Epoch 34/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
      "Epoch 00034: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 15s 131ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092\n",
      "Epoch 35/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
      "Epoch 00035: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 15s 131ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 36/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
      "Epoch 00036: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 15s 126ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0093 - val_mean_absolute_error: 0.0093\n",
      "Epoch 37/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
      "Epoch 00037: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 15s 129ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 38/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
      "Epoch 00038: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 16s 137ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 39/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00039: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 16s 136ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 40/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00040: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 15s 127ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 41/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00041: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 14s 120ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 42/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00042: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 14s 117ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 43/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00043: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 15s 127ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 44/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00044: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 17s 148ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092\n",
      "Epoch 45/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
      "Epoch 00045: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 15s 125ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092\n",
      "Epoch 46/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00046: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 15s 132ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 47/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00047: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 16s 138ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090\n",
      "Epoch 48/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00048: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 15s 130ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 49/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00049: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 15s 132ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 50/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00050: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 16s 140ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090\n",
      "Epoch 51/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00051: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 16s 139ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00052: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 16s 141ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092\n",
      "Epoch 53/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00053: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 16s 137ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0093 - val_mean_absolute_error: 0.0093\n",
      "Epoch 54/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00054: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 17s 149ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0093 - val_mean_absolute_error: 0.0093\n",
      "Epoch 55/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00055: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 16s 135ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092\n",
      "Epoch 56/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00056: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 16s 133ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 57/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00057: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 16s 133ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 58/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00058: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 16s 134ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092\n",
      "Epoch 59/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
      "Epoch 00059: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 16s 135ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092\n",
      "Epoch 60/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00060: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 17s 146ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092\n",
      "Epoch 61/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
      "Epoch 00061: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 16s 140ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 62/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00062: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 16s 134ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0093 - val_mean_absolute_error: 0.0093\n",
      "Epoch 63/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
      "Epoch 00063: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 15s 128ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 64/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00064: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 16s 138ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0093 - val_mean_absolute_error: 0.0093\n",
      "Epoch 65/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
      "Epoch 00065: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 16s 137ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090\n",
      "Epoch 66/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00066: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 17s 146ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 67/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00067: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 16s 137ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090\n",
      "Epoch 68/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00068: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 17s 141ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 69/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00069: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 16s 133ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 70/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00070: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 15s 125ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092\n",
      "Epoch 71/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00071: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 14s 120ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 72/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00072: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 15s 131ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 73/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00073: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 15s 132ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 74/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00074: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 15s 126ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 75/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
      "Epoch 00075: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 17s 149ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 76/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
      "Epoch 00076: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 17s 147ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0093 - val_mean_absolute_error: 0.0093\n",
      "Epoch 77/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
      "Epoch 00077: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 19s 160ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0094 - val_mean_absolute_error: 0.0094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
      "Epoch 00078: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 20s 171ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 79/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
      "Epoch 00079: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 18s 150ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0093 - val_mean_absolute_error: 0.0093\n",
      "Epoch 80/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
      "Epoch 00080: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 17s 143ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 81/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00081: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 17s 146ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 82/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00082: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 17s 145ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 83/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00083: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 15s 128ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092\n",
      "Epoch 84/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00084: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 14s 119ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0094 - val_mean_absolute_error: 0.0094\n",
      "Epoch 85/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00085: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 13s 115ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 86/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00086: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 13s 113ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090\n",
      "Epoch 87/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00087: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 14s 118ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090\n",
      "Epoch 88/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00088: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 16s 135ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0093 - val_mean_absolute_error: 0.0093\n",
      "Epoch 89/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00089: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 16s 134ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 90/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
      "Epoch 00090: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 15s 131ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 91/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00091: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 16s 134ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092\n",
      "Epoch 92/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
      "Epoch 00092: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 15s 126ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0094 - val_mean_absolute_error: 0.0094\n",
      "Epoch 93/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00093: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 16s 134ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090\n",
      "Epoch 94/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
      "Epoch 00094: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 15s 127ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 95/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00095: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 14s 120ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091\n",
      "Epoch 96/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
      "Epoch 00096: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 17s 142ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092\n",
      "Epoch 97/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00097: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 15s 130ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090\n",
      "Epoch 98/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
      "Epoch 00098: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 16s 136ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090\n",
      "Epoch 99/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0091 - mean_absolute_error: 0.0091\n",
      "Epoch 00099: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 14s 118ms/step - loss: 0.0091 - mean_absolute_error: 0.0091 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092\n",
      "Epoch 100/100\n",
      "116/117 [============================>.] - ETA: 0s - loss: 0.0092 - mean_absolute_error: 0.0092\n",
      "Epoch 00100: val_loss did not improve from 0.00901\n",
      "117/117 [==============================] - 15s 128ms/step - loss: 0.0092 - mean_absolute_error: 0.0092 - val_loss: 0.0090 - val_mean_absolute_error: 0.0090\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14366f476a0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y, batch_size=batch_size,epochs=100,verbose=1, validation_split=0.1,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_file = 'Weights-020--0.00901.hdf5' # choose the best checkpoint \n",
    "model.load_weights(weights_file) # load it\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258/258 [==============================] - 2s 9ms/step\n",
      "52/52 [==============================] - 0s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.predict_classes(train_x, verbose=1)\n",
    "pred = model.predict_classes(test_x, verbose=1)\n",
    "\n",
    "\n",
    "test['Age'] = pred\n",
    "test.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_0_0_20170103200522151.jpg.chip.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_0_0_20170110220447314.jpg.chip.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10_0_0_20170110221714752.jpg.chip.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10_0_0_20170110224524253.jpg.chip.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10_0_0_20170110224725285.jpg.chip - Copy.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1630</th>\n",
       "      <td>9_0_0_20170110225419379.jpg.chip.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1631</th>\n",
       "      <td>9_0_1_20170110224349623.jpg.chip.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1632</th>\n",
       "      <td>9_0_2_20170110225353988.jpg.chip.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1633</th>\n",
       "      <td>9_1_0_20170109203259973.jpg.chip.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1634</th>\n",
       "      <td>9_1_2_20161219204347420.jpg.chip.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1635 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         File_name  Age  Class\n",
       "0            10_0_0_20170103200522151.jpg.chip.jpg    0      0\n",
       "1            10_0_0_20170110220447314.jpg.chip.jpg    0      0\n",
       "2            10_0_0_20170110221714752.jpg.chip.jpg    0      0\n",
       "3            10_0_0_20170110224524253.jpg.chip.jpg    0      0\n",
       "4     10_0_0_20170110224725285.jpg.chip - Copy.jpg    0      0\n",
       "...                                            ...  ...    ...\n",
       "1630          9_0_0_20170110225419379.jpg.chip.jpg    0      0\n",
       "1631          9_0_1_20170110224349623.jpg.chip.jpg    0      0\n",
       "1632          9_0_2_20170110225353988.jpg.chip.jpg    0      0\n",
       "1633          9_1_0_20170109203259973.jpg.chip.jpg    0      0\n",
       "1634          9_1_2_20161219204347420.jpg.chip.jpg    0      0\n",
       "\n",
       "[1635 rows x 3 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('submission.csv')\n",
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [0]\n"
     ]
    }
   ],
   "source": [
    "i = random.choice(train.index)\n",
    "img_name = train.File_name[i]\n",
    "\n",
    "img = cv2.imread(os.path.join(data_dir, 'train', img_name)).astype('float32')\n",
    "#imshow(cv2.resize(img, (64, 64)))\n",
    "print('Predicted:',pred_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_image(ind,images_f,images_f_2,Model):\n",
    "  cv2_imshow(images_f[ind])\n",
    "  image_test=images_f_2[ind]\n",
    "  pred_1=Model.predict(np.array([image_test]))\n",
    "  #print(pred_1)\n",
    " # sex_f=['Male','Female']\n",
    "  age=int(np.round(pred_1[1][0]))\n",
    "  #sex=int(np.round(pred_1[0][0]))\n",
    "  print(\"Predicted Age: \"+ str(age))\n",
    "  #print(\"Predicted Sex: \"+ sex_f[sex])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images_f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-efb82f02c9db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'df'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimages_f\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimages_f_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'images_f' is not defined"
     ]
    }
   ],
   "source": [
    "test_image('df',images_f,images_f_2,Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12324325.jpg', 'df.jpg', 'fgfgbrgf.jpg']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\piyush.pathak\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1462 predict_function  *\n        return step_function(self, iterator)\n    C:\\Users\\piyush.pathak\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1452 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\piyush.pathak\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\piyush.pathak\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\piyush.pathak\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\piyush.pathak\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1445 run_step  **\n        outputs = model.predict_step(data)\n    C:\\Users\\piyush.pathak\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1418 predict_step\n        return self(x, training=False)\n    C:\\Users\\piyush.pathak\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:975 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs,\n    C:\\Users\\piyush.pathak\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:191 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer sequential_2 is incompatible with the layer: : expected min_ndim=4, found ndim=3. Full shape received: [32, 224, 3]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-ed9550f287b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m   \u001b[1;31m#  x=np.expand_dims(x,axis=0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mall_images\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalues\u001b[0m \u001b[1;32min\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[0;32m    129\u001b[0m           method.__name__))\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1597\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1598\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1599\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1600\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1601\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    812\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2826\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2828\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2829\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3208\u001b[0m           \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3209\u001b[0m           and call_context_key in self._function_cache.missed):\n\u001b[1;32m-> 3210\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_define_function_with_shape_relaxation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3139\u001b[0m           expand_composites=True)\n\u001b[0;32m   3140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3141\u001b[1;33m     graph_function = self._create_graph_function(\n\u001b[0m\u001b[0;32m   3142\u001b[0m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0;32m   3143\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\piyush.pathak\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1462 predict_function  *\n        return step_function(self, iterator)\n    C:\\Users\\piyush.pathak\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1452 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\piyush.pathak\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\piyush.pathak\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\piyush.pathak\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\piyush.pathak\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1445 run_step  **\n        outputs = model.predict_step(data)\n    C:\\Users\\piyush.pathak\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1418 predict_step\n        return self(x, training=False)\n    C:\\Users\\piyush.pathak\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:975 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs,\n    C:\\Users\\piyush.pathak\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:191 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer sequential_2 is incompatible with the layer: : expected min_ndim=4, found ndim=3. Full shape received: [32, 224, 3]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "#model1=load_model('model_vgg16.h5')\n",
    "model\n",
    "d={}\n",
    "dirr = r'C:\\Users\\piyush.pathak\\Desktop\\age prediction\\input\\new_data'\n",
    "os.chdir(dirr)\n",
    "all_images = os.listdir()\n",
    "print(all_images)\n",
    "for i in range(len(all_images)):\n",
    "    img=image.load_img(all_images[i],target_size=(224,224))\n",
    "    x=image.img_to_array(img)\n",
    "\n",
    "    x=x/255\n",
    "    x=np.expand_dims(x,axis=0)\n",
    "    model.predict(x)\n",
    "    d[all_images[i]]=np.argmax(model.predict(x), axis=1)\n",
    "for key,values in d.items():\n",
    "    print(str(key)+\"/\"+str(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
